#!/bin/bash

export USER=$(whoami)
system=$(uname -s)
if [ $system == Linux ]; then
  shopt -s checkwinsize
  alias ls='ls --color'
  alias xargs='xargs -r'
  toolchain-clang() {
    source ~/dev/hornet-opensource/thirdparty/toolchain-clang-$(uname -m)-Linux.sh
    export LDFLAGS="-Wl,-rpath,$DEPENDENCY_PATH/lib $LDFLAGS"
  }
  toolchain-gcc() {
    source ~/dev/hornet-opensource/thirdparty/toolchain-gcc-$(uname -m)-Linux.sh
  }
  yzy_CORE_DIR=$(dirname $(/sbin/sysctl -n kernel.core_pattern))
  yzy_NPROC=$(nproc)
fi
if [ $system == Darwin ]; then
  CLICOLOR=1 #for mac and BSD
  export LSCOLORS=gxfxcxdxbxegedabagacad
  alias toolchain='source /opt/dependency-Darwin/package/env.sh'
  if [[ -z $(which gls) ]]; then
    alias ls='ls -G'
  else
    alias ls='gls --color'
    date() {
      gdate $*
    }
  fi
  [[ -n $(which gecho) ]] && alias echo=gecho
  yzy_NPROC=$(sysctl -n hw.ncpu)
  yzy_CORE_DIR=$(dirname $(sysctl -n kern.corefile))
fi
trap 'echo -e "\033[31mERR $? in \033[1m${FUNCNAME-bash}: $BASH_COMMAND \033[0m"' ERR
# trap 'ts_start=$(date +%s%3N)' DEBUG
check() {
  echo -e "\033[32m$@\033[0m"
}
error() {
  echo -e "\033[31m$@\033[0m"
}
warning() {
  echo -e "\033[33m$@\033[0m"
}
export yzy_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]-$0}")/.." && pwd)"
export yzy_PATH="$(cd "$(dirname "${BASH_SOURCE[0]-$0}")" && pwd)"
HAWQ_SRC=~/dev/hawq
HORNET_SRC=~/dev/hornet
hornet_src_dir=~/dev/hornet/
hornet_cov_dir=~/dev/coverage/hornet/
export MAKEFLAGS=-j$yzy_NPROC

export PGDATABASE=postgres

export LESS=eFRX
export LANG=en_US
export LC_ALL=en_US.utf-8

export DEPENDENCY_PATH=/opt/dependency/package
export JAVA_LIBRARY_PATH=/usr/hdp/current/hadoop-client/lib/native/:/opt/dependency/package/lib/:/usr/local/lib:$JAVA_LIBRARY_PATH
export MACOSX_DEPLOYMENT_TARGET=10.12

export GOPATH=~/dev/goprojects
export PATH=$HOME/yizhiyang/bin:$PATH:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin

git-writer() {
  # git ls-tree -r -z --name-only HEAD -- $1 | xargs -0 -n1 git blame --line-porcelain HEAD | grep "^author-mail" | sort | uniq -c | sort -nr
  git grep -Il '' |
    xargs -n1 git blame --line-porcelain HEAD | grep "^author-mail" | sort | uniq -c | sort -nr
}

error_in_exciting() {
  if [[ $(hostname) =~ exciting ]]; then
    error "Do not do this in "
  fi
}

mytime() {
  ts_start=$(date +%s%3N)
  echo $@
  eval $@
  ts_end=$(date +%s%3N)
  t=$(expr $ts_end - $ts_start)
  echo $t ms
}

format-code() {
  start_sha=$(git merge-base $(git symbolic-ref refs/remotes/origin/HEAD) HEAD)
  git diff $start_sha --name-only --diff-filter=AM |
    grep -E '\.(h|c|cc|cpp)' |
    xargs clang-format -i
  git diff $start_sha --name-only --diff-filter=AM |
    grep -E '\.(h|c|cc|cpp)' |
    xargs cpplint.py
}

format-diff() {
  file1=$1
  file2=$2
  diff -u <(clang-format --style=google $file1) <(clang-format --style=google $file2)
}

find-latest() {
  find . -iname "$1" | xargs ls -ltr
}
find-latest-diff() {
  find . -iname '*.diff' | xargs ls -ltr
}
lldb-latest() {
  if [ -n "$1" ]; then
    lldb -c $(ls -rt ${yzy_CORE_DIR}/core.* | tail -n 1)
  else
    core_file=${yzy_CORE_DIR}/$(ls -1rt ${yzy_CORE_DIR}/ | tail -n 1)
    if [[ -f $core_file ]]; then
      ls -lhtr ${yzy_CORE_DIR}/
      echo
      lldb -c $core_file
    fi
  fi
}
alias lldb-recent='lldb -c ${yzy_CORE_DIR}/`ls -rt /cores| tail -n 2| head -n 1`'

[[ $(which vim) ]] && alias vi=vim
alias grep='grep --color'
alias vi-latest='vi `ls -1t|head -1`'
alias cutf="tr -s ' '| cut -d ' ' -f "

alias log-newqe='cd ~/dev/hawq/src/test/feature/newexecutor'
cd-featuretest() {
  if [ -d /usr/local/hawq/feature-test ]; then
    cd /usr/local/hawq/feature-test
  fi
  if [ -d ~/dev/hawq/src/test/feature ]; then
    cd ~/dev/hawq/src/test/feature
  fi
}
hawq-dir() {
  node=$1
  ps -eo pid,command | grep "[h]awq.*-M $node" | sed -E 's|.*-D ([^ ]+) .*|\1|'
}
yizhiyang-log-file() {
  hawq_component=$1
  hawq_node=$2
  log_dir=$(hawq-dir $hawq_node)/pg_log
  log_file=$(ls -tr $log_dir | grep $hawq_component | tail -n1)
  echo $log_dir/$log_file
}
alias dir-master='hawq-dir master'
alias dir-segment='hawq-dir segment'
alias tail-log-hawq-master='tail -f `yizhiyang-log-file hawq master`'
alias tail-log-magma-master='tail -f `yizhiyang-log-file magma master`'
alias tail-log-hawq-segment='tail -f `yizhiyang-log-file hawq segment` | grep --line-buffered --colour=never -v network_utils.c'
tail-log-hawq-cache() {
  tail -f $(yizhiyang-log-file hawq_cache segment)
}
alias tail-log-magma-segment='tail -f `yizhiyang-log-file magma segment`'
alias vi-log-hawq-master='vi `yizhiyang-log-file hawq master`'
alias vi-log-magma-master='vi `yizhiyang-log-file magma master`'
alias vi-log-hawq-segment='vi `yizhiyang-log-file hawq segment`'
alias vi-log-magma-segment='vi `yizhiyang-log-file magma segment`'
cd-log-hawq-master() {
  cd $(hawq-dir master)/pg_log
}
cd-log-hawq-segment() {
  cd $(hawq-dir segment)/pg_log
}
cd-log-namenode() {
  cd $(ps -eo pid,command | grep [p]roc_namenode | grep D | sed -E 's|.*-Dhadoop.log.dir=([^ ]+) .*|\1|')
}
cd-log-datanode() {
  cd $(ps -eo pid,command | grep [p]roc_datanode | grep D | sed -E 's|.*-Dhadoop.log.dir=([^ ]+) .*|\1|')
}
cat-log-hawq-master() {
  cat $(yizhiyang-log-file hawq master)
}
cat-log-hawq-segment() {
  cat $(yizhiyang-log-file hawq segment)
}

alias vi-tpch='vi ~/dev/hawq/src/test/feature/tpchtest/tpchorc_newqe.xml'
alias gpdb='source /usr/local/gpdb/greenplum_path.sh'
alias oushudb='source /usr/local/hawq/greenplum_path.sh'
alias apache-hawq='source /usr/local/apache-hawq/greenplum_path.sh'

alias apache-diff='git diff oushu/wcy-merge-apache'
alias apache-checkout='git checkout oushu/wcy-merge-apache'
alias fix='cp ~/FindSSL.cmake ./depends/libhdfs3/CMake/FindSSL.cmake && find . -name hawq_type_mapping.h| xargs rm'

#-------------------------------------------------------------------------------
#
# lava and ci
#
#-------------------------------------------------------------------------------
ci-get() {
  if [ -n "$1" ]; then
    lava scp -r \
      ciserver:/home/ec2-user/jenkins_home/.lava/machines/$1 \
      ~/.lava/machines
  fi
  find ~/.lava -name *.json | xargs sed -i '' "s|/var/jenkins_home/|$HOME/|g"
  find ~/.lava -name *.json | xargs sed -i '' "s|/Users/[^/]*/|$HOME/|g"
}
lava-clean() {
  nodes=$(lava ls | grep Error | tr -s ' ' | cut -d ' ' -f 1)
  for node in $nodes; do
    echo $node
    find ~/.lava -name $node | xargs rm -rf
  done
}

#-------------------------------------------------------------------------------
#
#   Hawq
#
#-------------------------------------------------------------------------------
alias hawq-config='hawq-config.py'
hawq-config-deprecated() {
  if [[ $# -eq 1 ]]; then
    hawq config --show $1
  else
    hawq config --skipvalidation --change $1 --value $2
  fi
}
hawq-config-load() {
  echo 'hawq-config-load'
  if [[ -n $project_prefix || $(hostname) =~ ^exciting ]]; then
    warning 'skip hawq-config-load yizhiyangrc in exciting'
  else
    check "source $yzy_PATH/yizhiyangrc"
    source $yzy_PATH/yizhiyangrc
  fi
  for hawq_config in $(printenv | grep -E ^hawq_config_[^=]+=); do
    name_value=${hawq_config#hawq_config_}
    name=${name_value%%=*}
    value=${name_value#*=}
    echo "|   hawq-config $name $value"
    hawq-config $name $value
  done
  echo '+---------------'
}
hawq-qe() {
  if [ -n "$1" ]; then
    ps -eo pid,command | grep [p]ostgres.*con.*seg | awk '{print $1}' |
      sort | head -n $1 | tail -n 1
  else
    ps -eo pid,command | grep [p]ostgres.*con.*seg | awk '{print $1}' |
      sort
  fi
}
hawq-qd() {
  ps -eo pid,command | grep [p]ostgres.*con.*cmd | awk '{print $1}'
}
hawq-perf() {
  t=100
  tt=0
  while [[ -z $(hawq-qe) || $t -ne $tt ]]; do
    t=$tt
    tt=$(hawq-qe | wc -l)
  done
  hawq-qe |
    awk 'BEGIN {ORS=","} {print $1}' |
    xargs perf record --call-graph dwarf $@ -p
}
hawq-instrument() { # timing in milisecond
  instruments -l $1 -t 'Time Profiler' -p $(hawq-qe 2)
}

magma-clean() {
  set -x
  sudo rm -rf $(hawq-config hawq_magma_locations_master | sed -E 's|[^:,]+://||g' |
    sed 's|,| |g')
  sudo rm -rf $(hawq-config hawq_magma_locations_segment | sed -E 's|[^:,]+://||g' |
    sed 's|,| |g')
  set +x
}
hawq-clean() {
  set -x
  sudo rm -rf $(hawq-config hawq_master_directory)
  sudo rm -rf $(hawq-config hawq_segment_directory)
  sudo rm -rf /tmp/.*PGSQL*
  sudo rm -rf /tmp/pgsql_tmp
  sudo rm -rf /tmp/checktmpdir.log
  sudo rm -rf /tmp/Test* /tmp/test* /tmp/magma* /tmp/clusterview/ /tmp/catalogut /tmp/catalogrpc /tmp/rg* /tmp/range* /tmp/err_table
  if [[ -n $(hawq-config hawq_dfs_url) && none != $(hawq-config hawq_dfs_url) ]]; then
    if [[ $system == Linux && $(getent passwd hdfs) ]]; then
      sudo -iu hdfs hdfs dfs -rm -r -f hdfs://$(hawq-config hawq_dfs_url)/*
    else
      hdfs dfs -rm -r -f hdfs://$(hawq-config hawq_dfs_url)/*
    fi
  fi
  set +x
}
magma-init() {
  hawq sql -c "drop database hawq_feature_test_db;"
  hawq-setup-feature-test
  magma-clean
  hawq-restart
}
hawq-init() { (
  set -ex
  num=${1}
  hawq-stop
  magma-clean
  hawq-clean

  if [[ -n $num ]]; then
    hawq-config default_hash_table_bucket_number $num
    hawq-config default_magma_hash_table_nvseg_per_node $num
    hawq-config hawq_rm_nvseg_perquery_perseg_limit $num
    # hawq-config magma_range_number $num
  fi

  # the following clean job remove the content inside the new config
  hawq-config-load
  magma-clean
  hawq-clean

  sudo install -o $USER -d $(hawq-config hawq_magma_locations_master |
    sed -E 's|[^:,]+://||g' |
    sed 's|,| |g')
  sudo install -o $USER -d $(hawq-config hawq_magma_locations_segment |
    sed -E 's|[^:,]+://||g' |
    sed 's|,| |g')
  sudo install -o $USER -d $(hawq-config hawq_master_directory)
  sudo install -o $USER -d $(hawq-config hawq_segment_directory)

  if [[ $(hawq-config hawq_init_with_magma | uniq) == true ]]; then
    with_magma='--with_magma'
  else
    with_magma=''
  fi
  hawq init cluster -a --locale='C' --lc-collate='C' \
    --lc-ctype='C' --lc-messages='C' \
    --lc-monetary='C' --lc-numeric='C' \
    --lc-time='C' ${with_magma}
  hawq-setup-feature-test && rm -rf ~/hawqAdminLogs
  set +ex
); }
hawq-stop() {
  sudo pkill -9 'pg_ctl|postgres|magma_server|gpfdist|feature-test|-unit' || true
  sudo pkill -9 hawq_ctl || true
  return
  ps -eo pid,command | grep -E '^ *[0-9]+ [^ ]*gtest-parallel' | awk '{print $1}' | xargs sudo kill -9
  ps -eo pid,command | grep -E '^ *[0-9]+ [^ ]*feature-test' | awk '{print $1}' | xargs sudo kill -9
  ps -eo pid,command | grep -E '^ *[0-9]+ [^ ]*postgres' | awk '{print $1}' | xargs sudo kill -9
  ps -eo pid,command | grep -E '^ *[0-9]+ [^ ]*magma_server' | awk '{print $1}' | xargs sudo kill -9
  ps -eo pid,command | grep -E '^ *[0-9]+ [^ ]*gpfdist' | awk '{print $1}' | xargs sudo kill -9
}
hawq-restart() {
  if [ "$#" -eq 1 ]; then
    local val=$1
    echo "set QE num to $1"
    hawq-config hawq_rm_nvseg_perquery_perseg_limit $val
    hawq-config default_hash_table_bucket_number $val
    hawq-config default_magma_hash_table_nvseg_per_seg $val
    hawq-config hawq_rm_nvseg_for_analyze_part_perquery_perseg_limit $val
  else
    hawq-config hawq_rm_nvseg_perquery_perseg_limit
    hawq-config default_hash_table_bucket_number
    hawq-config default_magma_hash_table_nvseg_per_seg
    hawq-config hawq_rm_nvseg_for_analyze_part_perquery_perseg_limit $val
  fi
  hawq-config-load
  hawq-stop
  mkdir -p $(hawq-config hawq_master_temp_directory)
  mkdir -p $(hawq-config hawq_segment_temp_directory)

  if [[ $(hawq-config hawq_init_with_magma | uniq) == true ]]; then
    with_magma='--with_magma'
  else
    with_magma=''
  fi
  hawq restart cluster -a -M immediate $with_magma
  if [[ -n $val ]]; then
    psql -ac "ALTER VCLUSTER vc_default with (hash_table_bucket_number=$val);"
    psql -ac "ALTER VCLUSTER vc_default with (max_nvseg_perquery_perseg=$val);"
    psql -ac "ALTER VCLUSTER vc_default with (magma_hash_table_nvseg_perseg=$val);"
  fi
}
hawq-setup-feature-test() {
  TEST_DB_NAME="hawq_feature_test_db"
  source /usr/local/hawq/greenplum_path.sh
  psql -d postgres <<EOF
  drop database if exists $TEST_DB_NAME;
  create database $TEST_DB_NAME;
  alter database $TEST_DB_NAME set lc_messages to 'C';
  alter database $TEST_DB_NAME set lc_monetary to 'C';
  alter database $TEST_DB_NAME set lc_numeric to 'C';
  alter database $TEST_DB_NAME set lc_time to 'C';
  alter database $TEST_DB_NAME set timezone_abbreviations to 'Default';
  alter database $TEST_DB_NAME set timezone to 'PST8PDT';
  alter database $TEST_DB_NAME set datestyle to 'postgres,MDY';
EOF
}
checkout-hawq-test() {
  ftest="$HAWQ_SRC/src/test/feature/feature-test"
  if [[ -f /usr/local/hawq/feature-test/feature-test ]]; then
    ftest="/usr/local/hawq/feature-test/feature-test"
  elif [[ -d ~/dev/oushudb/cmake-build-debug ]]; then
    echo -e "Building cmake/feature-test \e[5m...\e[0m\n"
    make -C ~/dev/oushudb/cmake-build-debug feature-test
    if [[ $? -ne 0 ]]; then
      return 1
    fi
  else
    echo -e "Building $HAWQ_SRC/feature-test \e[5m...\e[0m\n"
    make -C $HAWQ_SRC feature-test
    if [[ $? -ne 0 ]]; then
      return 1
    fi
  fi
  check $ftest
}
hawq-test-list() {
  checkout-hawq-test || return 1
  $ftest --gtest_list_tests | awk '$1 ~ /\./ {kk=$1; print kk}; $1 !~ /\./  {print "  " kk $1}'

  echo
  check $ftest
}
hawq-test() {
  checkout-hawq-test || return 1
  TEST_DB_NAME="hawq_feature_test_db"
  export PGDATABASE=$TEST_DB_NAME
  if [ -n "$1" ]; then
    bash -c "
    source /usr/local/hawq/greenplum_path.sh
    $ftest --gtest_filter=$1"
  else
    bash -c "
    source /usr/local/hawq/greenplum_path.sh
    $ftest --gtest_filter=TestNewExec*:-*MagmaTP*:*MagmaAP*:*TestS3*"
  fi

  # export PGDATABASE=postgres
}

#-------------------------------------------------------------------------------
#
#  Hornet
#
#-------------------------------------------------------------------------------
export RUN_UNITTEST=no
hornet-build() {
  cmake_dir=~/dev/oushudb/cmake-build-${1-debug}
  if [[ $WITH_HORNET_EXECUTOR_ONLY == yes ]]; then
    make executor-shared -C $cmake_dir/
    make install -C $cmake_dir/hornet-opensource/
    make install -C $cmake_dir/hornet/interconnect
    make install -C $cmake_dir/hornet/executor
    install -d /opt/dependency/include/magma/
    touch /opt/dependency/package/include/magma/cwrapper/magma-client-c.h
    touch /opt/dependency/package/include/storage/cwrapper/magma-format-c.h
    return 0
  fi
  make install -C $cmake_dir/hornet/
}
hornet-debug() {
  # cd ~/dev/hornet && make incremental && cd -
  hornet-build debug
}
hornet-release() {
  # cd ~/dev/release/hornet && make incremental && cd -
  hornet-build release
}
hornet-unittest() {
  cd ~/dev/hornet && RUN_UNITTEST=YES make incremental && cd -
}

hornet-coverage-setup() {
  # FIXME(chiyang): conflicts with running feature-test
  # sudo rm -rf /tmp/Test* /tmp/test* /tmp/err_table
  # sudo rm -rf /tmp/magma* /tmp/clusterview/ /tmp/catalog* /tmp/rg* /tmp/range*

  mkdir -p ${hornet_cov_dir}
  cd ${hornet_cov_dir} && ${hornet_src_dir}/bootstrap
  sed -i.bak "s|PREFIX=.*|PREFIX=${hornet_cov_dir}/dependency|g" build-all.sh
  sed -i.bak "s|DEPENDENCY_INSTALL_PREFIX=.*|DEPENDENCY_INSTALL_PREFIX=${hornet_cov_dir}/dependency|g" build-all.sh
  sed -i.bak "s|make coverage|make -j$yzy_NPROC install unit|g" build-all.sh
  sed -i.bak '45a \
    OPTION=--enable-coverage
    ' build-all.sh
  sed -i.bak '68d' build-all.sh
}
hornet-coverage-report() {
  arg=$1
  [[ -z $@ ]] && arg=gcovr
  [[ -z $(which gcovr) ]] && arg=lcov
  echo -e "Invoking $arg \e[5m...\e[0m\n"

  if [[ $arg == gcovr ]]; then
    time gcovr -j$yzy_NPROC \
      --root ${hornet_src_dir} \
      ${hornet_cov_dir}/ -o ${hornet_cov_dir}/hornet.gcovr
    parse_gcovr_output.py ${hornet_src_dir} ${hornet_cov_dir}/hornet.gcovr |
      tee ${hornet_cov_dir}/hornet.report
    return 0
    # --exclude-directories='.*/test/unit' \
  fi

  cd ${hornet_cov_dir}/
  time lcov --base-directory . --directory . --capture --output-file CodeCoverage.info --ignore-errors graph
  lcov --remove CodeCoverage.info '/opt/*' '/usr/*' '/Library/*' '/Applications/*' \
    '*/build/src/storage/format/orc/*' \
    '*/protos/*' '*/proto/*' '*/thrift/*' \
    --output-file CodeCoverage.info.cleaned
  parse_lcov_output.py ${hornet_src_dir} ${hornet_cov_dir}/CodeCoverage.info.cleaned |
    tee ${hornet_cov_dir}/hornet.report
  # '*/test/unit/*' '*/testutil/*' \
}
hornet-coverage() { (
  set -e
  test $# -gt 2 && error 'Invalid number of arguments.' && return 1
  if [[ $# -eq 0 || $# -eq 2 ]]; then
    if [[ $# -eq 2 ]]; then
      hornet_src_dir=$1
      hornet_cov_dir=$2
    fi
    hornet-coverage-setup
    cd ${hornet_cov_dir} && make incremental
    for m in dbcommon univplan interconnect magma storage executor; do
      cd ${hornet_cov_dir}/$m/build
      make resetcoverage
      make punittest
    done
    hornet-coverage-report
    return 0
  fi
  case $1 in
  dbcommon) ;;
  univplan) ;;
  interconnect) ;;
  storage) ;;
  executor) ;;
  *)
    error Invalid component $1
    return 1
    ;;
  esac
  # hornet-debug
  hornet-coverage-setup
  component=$1
  cd ${hornet_cov_dir}
  mkdir -p ${hornet_cov_dir}/$component/build
  cd ${hornet_cov_dir}/$component/build
  ${hornet_src_dir}/$component/bootstrap --enable-coverage
  make resetcoverage
  make punittest
  gen-coverage
); }
hornet-test() {
  m=$(pwd | sed -E 's|.*/([^/]+)/build|\1|')
  if [ -n "$1" ]; then
    if [[ $(pwd) =~ "magma" ]]; then
      make $m-unit && test/unit/magma_server/unit --gtest_catch_exceptions=0 --gtest_filter=$1
    else
      make $m-unit && test/unit/$m-unit --gtest_catch_exceptions=0 --gtest_filter=$1
    fi
  else
    make $m-unit && make punittest
  fi
}
hornet-test-list() {
  m=$(pwd | sed -E 's|.*/([^/]+)/build|\1|')
  if [[ $(pwd) =~ "magma" ]]; then
    make unit && test/unit/magma_server/unit --gtest_list_tests
  else
    make $m-unit && test/unit/$m-unit --gtest_list_tests
  fi
}
hornet-test-lldb() {
  m=$(pwd | sed -E 's|.*/([^/]+)/build|\1|')
  if [[ $(pwd) =~ "magma" ]]; then
    make unit && lldb test/unit/magma_server/unit -- --gtest_filter=$1
  else
    make $m-unit && lldb test/unit/$m-unit -- --gtest_filter=$1
  fi
}
hornet-test-gdb() {
  m=$(pwd | sed -E 's|.*/([^/]+)/build|\1|')
  if [[ $(pwd) =~ "magma" ]]; then
    make unit && gdb --args test/unit/magma_server/unit --gtest_filter=$1
  else
    make $m-unit && gdb --args test/unit/$m-unit --gtest_filter=$1
  fi
}
gen-coverage() {
  lcov --base-directory . --directory . --capture --output-file CodeCoverage.info --ignore-errors graph
  lcov --remove CodeCoverage.info '/opt/*' '/usr/*' '/Library/*' '/Applications/*' \
    '*/build/src/storage/format/orc/*' \
    '*/test/unit/*' '*/testutil/*' \
    '*/protos/*' '*/proto/*' '*/thrift/*' \
    --output-file CodeCoverage.info.cleaned
  genhtml CodeCoverage.info.cleaned -o CodeCoverageReport
  test -d /var/www/html/ && sudo cp -r CodeCoverageReport/* /var/www/html/
  [[ $system == Darwin ]] && open CodeCoverageReport/index.html
}

#-------------------------------------------------------------------------------
#
#   Docker on Hawq
#
#-------------------------------------------------------------------------------
yizhiyang-start() {
  sudo docker run -d -t --entrypoint bash \
    -v /var/run/docker.sock:/var/run/docker.sock \
    -v $HOME/dev-linux/dependency:/root/dependency \
    -v $HOME/.m2:/root/.m2 \
    -v $HOME/dev-linux/:/root/hawq \
    -v $HOME/dev/:/root/dev \
    -e BUILD_OPTION=debug \
    -e BUILD_NUMBER=38324 \
    -e DB_VERSION=14122 \
    -e MAKE_COMPONET=hawq \
    -e OVERWRITE_OPTION=YES \
    -e PS1='[\u@\h \w]\$ ' \
    --name yizhiyang \
    --privileged=true hub.oushu-tech.com/hawq_compile:v4.0.0.0
}
yizhiyang-login() {
  sudo docker exec -it yizhiyang /bin/bash
}
yizhiyang-stop() {
  sudo docker stop yizhiyang
  sudo docker rm yizhiyang
}
docker-clean() {
  docker ps -a | grep Dead | cut -d ' ' -f 1 | xargs docker rm
  docker ps -a | grep Exit | cut -d ' ' -f 1 | xargs docker rm
  docker images | grep none | tr -s ' ' | cut -d ' ' -f 3 | xargs docker rmi
}
docker-stats() {
  docker stats --format="table {{.Name}}\t{{.CPUPerc}}\t{{.MemPerc}}\t{{.NetIO}}\t{{.BlockIO}}\t{{.PIDs}}"
}
docker-ps() {
  docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Image}}"
}
saiting-ps() {
  # docker stats  --no-stream --format="table {{.Name}}\t{{.CPUPerc}}\t{{.MemPerc}}\t{{.PIDs}}"
  echo
  docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Image}}"
  echo
  echo "欢迎来到一颗赛艇！🛶⛵️🚤🛥 🛳 ⛴ 🚢"
  echo -e '
\033[32m
赛艇管理员提醒您，直接执行以下命令配置您的本地机器的SSH客户端，
将有助于长时间保持SSH连接，大大减少正常操作过程中Connection closed by的情况。

\033[33m
cat >>~/.ssh/config <<EOF
Host *
ServerAliveInterval 60
ServerAliveCountMax 600
EOF
\033[0m'
}
saiting-login() {
  docker exec $* sudo -u gpadmin touch /home/gpadmin/yikesaiting_chiyang
  docker exec -it $* su gpadmin
}

#-------------------------------------------------------------------------------
#
#   Basic
#
#-------------------------------------------------------------------------------
# Green for Linux, yellow for macOS.
PS1='\[\033[00m\][\t] ' # time
if [[ $system == Darwin ]]; then # user and host
  PS1=$PS1'\[\033[01;33m\]\u\[\033[00m\]@\[\033[33m\]\h\[\033[00m\]'
elif [[ $(hostname) =~ exciting ]]; then
  if [ -t 0 ]; then
    check "赛艇管理员提醒您"
    warning "toolchain-clang 使用Clang工具链"
    warning "toolchain-gcc   使用GCC工具链"
  fi
  PS1=$PS1'\[\033[01;36m\]\u\[\033[00m\]@\[\033[36m\]\h\[\033[00m\]'
else
  PS1=$PS1'\[\033[01;32m\]\u\[\033[00m\]@\[\033[32m\]\h\[\033[00m\]'
fi
PS1=$PS1':\[\033[01;34m\]\w\[\033[00m\]' # working directory

# Disable git prompt in docker to improve response speed.
if [[ -f ~/yizhiyang/config/git-completion.bash && -f ~/yizhiyang/config/git-prompt.sh && -z "${I_AM_DOCKER+x}" ]]; then
  source ~/yizhiyang/config/git-completion.bash
  source ~/yizhiyang/config/git-prompt.sh
  export GIT_PS1_SHOWDIRTYSTATE=true
  export GIT_PS1_SHOWUNTRACKEDFILES=true
  export GIT_PS1_SHOWUPSTREAM="verbose name"
  PS1=$PS1'`__git_ps1 " (%s)"`'
fi
# last command status
export PS1=$PS1'\n\[\033[01m\]\$ \[\033[00m\]' # UID symbol
export PS4='🐤♻️  '

export LS_COLORS="rs=0:di=38;5;27:ln=38;5;51:mh=44;38;5;15:pi=40;38;5;11:so=38;5;13:do=38;5;5:bd=48;5;232;38;5;11:cd=48;5;232;38;5;3:or=48;5;232;38;5;9:mi=05;48;5;232;38;5;15:su=48;5;196;38;5;15:sg=48;5;11;38;5;16:ca=48;5;196;38;5;226:tw=48;5;10;38;5;16:ow=48;5;10;38;5;21:st=48;5;21;38;5;15:ex=38;5;34:*.tar=38;5;9:*.tgz=38;5;9:*.arc=38;5;9:*.arj=38;5;9:*.taz=38;5;9:*.lha=38;5;9:*.lz4=38;5;9:*.lzh=38;5;9:*.lzma=38;5;9:*.tlz=38;5;9:*.txz=38;5;9:*.tzo=38;5;9:*.t7z=38;5;9:*.zip=38;5;9:*.z=38;5;9:*.Z=38;5;9:*.dz=38;5;9:*.gz=38;5;9:*.lrz=38;5;9:*.lz=38;5;9:*.lzo=38;5;9:*.xz=38;5;9:*.bz2=38;5;9:*.bz=38;5;9:*.tbz=38;5;9:*.tbz2=38;5;9:*.tz=38;5;9:*.deb=38;5;9:*.rpm=38;5;9:*.jar=38;5;9:*.war=38;5;9:*.ear=38;5;9:*.sar=38;5;9:*.rar=38;5;9:*.alz=38;5;9:*.ace=38;5;9:*.zoo=38;5;9:*.cpio=38;5;9:*.7z=38;5;9:*.rz=38;5;9:*.cab=38;5;9:*.jpg=38;5;13:*.jpeg=38;5;13:*.gif=38;5;13:*.bmp=38;5;13:*.pbm=38;5;13:*.pgm=38;5;13:*.ppm=38;5;13:*.tga=38;5;13:*.xbm=38;5;13:*.xpm=38;5;13:*.tif=38;5;13:*.tiff=38;5;13:*.png=38;5;13:*.svg=38;5;13:*.svgz=38;5;13:*.mng=38;5;13:*.pcx=38;5;13:*.mov=38;5;13:*.mpg=38;5;13:*.mpeg=38;5;13:*.m2v=38;5;13:*.mkv=38;5;13:*.webm=38;5;13:*.ogm=38;5;13:*.mp4=38;5;13:*.m4v=38;5;13:*.mp4v=38;5;13:*.vob=38;5;13:*.qt=38;5;13:*.nuv=38;5;13:*.wmv=38;5;13:*.asf=38;5;13:*.rm=38;5;13:*.rmvb=38;5;13:*.flc=38;5;13:*.avi=38;5;13:*.fli=38;5;13:*.flv=38;5;13:*.gl=38;5;13:*.dl=38;5;13:*.xcf=38;5;13:*.xwd=38;5;13:*.yuv=38;5;13:*.cgm=38;5;13:*.emf=38;5;13:*.axv=38;5;13:*.anx=38;5;13:*.ogv=38;5;13:*.ogx=38;5;13:*.aac=38;5;45:*.au=38;5;45:*.flac=38;5;45:*.mid=38;5;45:*.midi=38;5;45:*.mka=38;5;45:*.mp3=38;5;45:*.mpc=38;5;45:*.ogg=38;5;45:*.ra=38;5;45:*.wav=38;5;45:*.axa=38;5;45:*.oga=38;5;45:*.spx=38;5;45:*.xspf=38;5;45:"

# Iterm
test -e "${HOME}/.iterm2_shell_integration.bash" && source "${HOME}/.iterm2_shell_integration.bash"
export PROMPT_COMMAND='echo -ne "\033];${PWD##/Users/admin/}\007";'

if [[ -t 0 && $(hostname) == yikesaiting ]]; then
  saiting-ps
  complete -W "$(saiting-ps | awk '/^exci/{print $1}')" saiting-login
fi

#-------------------------------------------------------------------------------
#
#   Completion
#
#-------------------------------------------------------------------------------
_make_completions() {
  targets=$(make help | awk '/... /{print $2}')
  [ "${#COMP_WORDS[@]}" == 2 ] && COMPREPLY=($(compgen -W "$targets" "${COMP_WORDS[1]}"))
}
# complete -F _make_completions make
